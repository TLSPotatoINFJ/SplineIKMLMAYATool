function train_deformer_mapper_from_npz(rootDir, outDir)
% è¯»å– Maya å•æ¡æ ·æœ¬ npzï¼ˆX_mats/Y_mats/X_vec60/Y_vec36/flatOrderï¼‰ï¼Œ
% ä¸å½’ä¸€åŒ–/SVDï¼Œè®­ç»ƒ 60->36 MLPï¼Œå¹¶å¯¼å‡ºå¯¹ç…§ npzã€‚
% åŠ å¼ºç¨³å®šæ€§ï¼šæ›´å°å­¦ä¹ ç‡ + ä½™å¼¦é€€ç« + æ¢¯åº¦è£å‰ª + æ ·æœ¬ä¸€è‡´æ€§è‡ªæ£€ã€‚

arguments
    rootDir (1,1) string
    outDir  (1,1) string
end

%% ====== é…ç½® ======
cfg.forceFlatOrder    = "";    % è®¾ä¸º "row" å¯å¼ºåˆ¶è¡Œå±•å¹³ï¼›é»˜è®¤ "" ä¸å¼ºåˆ¶
cfg.roundOutput       = false; % å¯¼å‡º npz æ˜¯å¦ round
cfg.roundDigits       = 7;
cfg.consistencyThresh = 1e-3;  % Y_mats ä¸ Y_vec36 æœ€å¤§ç»å¯¹å·®é˜ˆå€¼ï¼ˆè¶…å‡ºæŠ¥é”™ï¼‰

nn.hidden      = [512 128];
nn.numEpochs   = 6000;      % å¤Ÿç”¨å³å¯ï¼Œä½™å¼¦é€€ç«ä¼šæŠŠ lr æ”¶å°¾
nn.miniBatch   = 32;
nn.baseLR      = 1e-4;      % â†“ æ›´å°å­¦ä¹ ç‡ï¼ˆå…³é”®ï¼‰
nn.minLR       = 1e-10;
nn.scheduler   = "cosine";
nn.stepIters   = 2000;      %#ok<NASGU>  % é¢„ç•™ç»™ step ç­–ç•¥ï¼Œä¸ç”¨
nn.stepGamma   = 0.5;       %#ok<NASGU>
nn.l2Factor    = 0;
nn.dropoutRate = 0.0;
nn.earlyStopOn = false;
nn.clipNorm    = 1.0;       % å…¨å±€ L2 æ¢¯åº¦è£å‰ª

%% ====== åŸºç¡€æ£€æŸ¥ ======
if ~isfolder(rootDir), error("rootDir ä¸å­˜åœ¨: %s", rootDir); end
if ~isfolder(outDir),  mkdir(outDir); end
files = dir(fullfile(rootDir, "*.npz"));
if isempty(files), error("ç›®å½• %s ä¸‹æ²¡æœ‰ .npz", rootDir); end
fprintf("Found %d npz samples.\n", numel(files));

%% ====== è¯»å…¥ + ä¸€è‡´æ€§æ£€æŸ¥ ======
Xall = []; Yall = [];
flatOrder_all = "row";
sampleStructs = {};

badList = strings(0,1);

for i = 1:numel(files)
    fpath = fullfile(files(i).folder, files(i).name);
    try
        s = read_single_npz_maya_style(fpath);  % struct: x60, y36, Xm, Ym, flatOrder
        % å¯é€‰ï¼šå¼ºåˆ¶ç»Ÿä¸€å±•å¹³æ–¹å¼
        if ~isempty(cfg.forceFlatOrder)
            s.flatOrder = string(cfg.forceFlatOrder);
            s.x60 = pack_input_60_from_mats(s.Xm, s.flatOrder);
            s.y36 = pack_3mats_to_vec36_from_mats(s.Ym, s.flatOrder);
        end
        % ä¸€è‡´æ€§è‡ªæ£€ï¼šç”¨ Ym é‡æ–° pack æˆ y36_checkï¼Œçœ‹çœ‹ä¸æ–‡ä»¶ä¸­çš„ y36 å·®å¤šå°‘
        y36_check = pack_3mats_to_vec36_from_mats(s.Ym, s.flatOrder);
        dmax = max(abs(y36_check(:) - s.y36(:)));
        if dmax > cfg.consistencyThresh
            badList(end+1) = string(files(i).name); %#ok<AGROW>
        end
    catch ME
        warning("è·³è¿‡æ ·æœ¬ %s: %s", fpath, ME.message);
        continue
    end

    if isempty(Xall)
        Xall = s.x60(:);
        Yall = s.y36(:);
        flatOrder_all = s.flatOrder;
    else
        Xall(:,end+1) = s.x60(:); %#ok<AGROW>
        Yall(:,end+1) = s.y36(:); %#ok<AGROW>
    end
    sampleStructs{end+1} = s; %#ok<AGROW>
end

if ~isempty(badList)
    fprintf("[WARN] å‘ç° %d æ¡æ ·æœ¬çš„ Y_mats ä¸ Y_vec36 ä¸ä¸€è‡´ï¼ˆ>|Î”|max>%.3gï¼‰\n", ...
        numel(badList), cfg.consistencyThresh);
    disp(badList);
    error("å­˜åœ¨ä¸ä¸€è‡´çš„æ ·æœ¬ï¼Œè¯·å…ˆä¿®å¤è¯¥æ•°æ®æºã€‚");
end

N = size(Xall,2);
if N==0, error("æœ‰æ•ˆæ ·æœ¬ä¸º 0ï¼Œæ£€æŸ¥ .npz æ ¼å¼ã€‚"); end
fprintf("Loaded %d valid samples.\n\n", N);

%% ====== è®­ç»ƒå‰ï¼šæ‰“å°ä¸€æ¡æœªç¼–è¾‘åŸæ ·æœ¬çš„å®Œæ•´ y_vec36 ======
idx_for_print = pick_sample_by_yvec_has_translation(Yall);
s_print = sampleStructs{idx_for_print};
fprintf("[DEBUG] ---- åŸå§‹æ ·æœ¬ #%d çš„å®Œæ•´ y_vec36ï¼ˆæœªåšç¼–è¾‘ï¼‰----\n", idx_for_print);
disp(s_print.y36(:));
fprintf("[DEBUG] -------------------------------------------------\n\n");

%% ====== åˆ’åˆ†ï¼ˆç¨³å¥ï¼‰ ======
perm = randperm(N);
if N == 1
    valIdx = 1; trnIdx = [];
else
    valRatio = 0.1;
    nVal = max(1, min(round(N*valRatio), N-1));
    valIdx = perm(1:nVal);
    trnIdx = perm(nVal+1:end);
end

Xtr = single(Xall(:,trnIdx));  Ytr = single(Yall(:,trnIdx));
Xv  = single(Xall(:,valIdx));  Yv  = single(Yall(:,valIdx));

%% ====== GPU ======
useGPU = canUseGPU;
if useGPU
    gpuDevice([]); fprintf("ğŸŸ¢ Using GPU for training.\n");
else
    fprintf("âšª GPU not available, using CPU.\n");
end

%% ====== ç½‘ç»œ ======
layers = [
    featureInputLayer(60,"Name","in","Normalization","none")
];
for i = 1:numel(nn.hidden)
    h = nn.hidden(i);
    layers = [layers; fullyConnectedLayer(h,"Name","fc"+i); reluLayer("Name","relu"+i)];
    if nn.dropoutRate>0
        layers = [layers; dropoutLayer(nn.dropoutRate,"Name","drop"+i)]; %#ok<AGROW>
    end
end
layers = [layers; fullyConnectedLayer(36,"Name","fc_out")];

lgraph = layerGraph(layers);
net    = dlnetwork(lgraph);
if useGPU
    net = dlupdate(@gpuArray, net);
    Xv  = gpuArray(Xv);
    Yv  = gpuArray(Yv);
end

%% ====== è®­ç»ƒè¿›åº¦å›¾ ======
fig = figure('Name','Training Progress','NumberTitle','off');
ax  = axes(fig); hold(ax,'on'); grid(ax,'on');
yyaxis left;
hTrain = animatedline('LineWidth',1.6,'DisplayName','Train MSE');
hVal   = animatedline('LineWidth',1.6,'DisplayName','Val MSE');
ylabel('Loss (MSE)');
yyaxis right;
hLR    = animatedline('LineWidth',1.2,'DisplayName','LR');
ylabel('Learning Rate');
xlabel('Iteration');
legend('show','Location','northeast');
title('Train / Val Loss & LR');
drawnow;

%% ====== è®­ç»ƒï¼ˆå¸¦æ¢¯åº¦è£å‰ªï¼‰ ======
beta1=0.9; beta2=0.999; epsilon=1e-8;
avgGrad=[]; avgSqGrad=[];
itersPerEpoch = max(1, ceil(max(1,size(Xtr,2))/max(1,nn.miniBatch)));
totalItersEst = max(1, nn.numEpochs * itersPerEpoch);
iter=0; logI=[]; logT=[]; logV=[]; logLR=[];

for e = 1:nn.numEpochs
    if ~isempty(trnIdx)
        ord = randperm(size(Xtr,2));
        Xtr = Xtr(:,ord); Ytr = Ytr(:,ord);
    end

    for t = 1:max(1,nn.miniBatch):max(1,size(Xtr,2))
        if isempty(trnIdx)
            Xb = Xv; Yb = Yv;   % ä»…ä¸ºå¯è§†åŒ–è¿è´¯
        else
            sel = t:min(t+nn.miniBatch-1, size(Xtr,2));
            Xb  = Xtr(:,sel); Yb=Ytr(:,sel);
        end
        if useGPU, Xb=gpuArray(Xb); Yb=gpuArray(Yb); end

        lr = scheduleLR(nn, iter+1, totalItersEst);

        [grad,loss] = dlfeval(@modelGradientsL2, net, dlarray(Xb,"CB"), dlarray(Yb,"CB"), nn.l2Factor);
        % ---- å…¨å±€ L2 æ¢¯åº¦è£å‰ª ----
        gn = globalGradL2Norm(grad);
        if gn > nn.clipNorm
            scale = nn.clipNorm / (gn + 1e-12);
            grad = dlupdate(@(g) g*scale, grad);
        end

        [net,avgGrad,avgSqGrad] = adamupdate(net,grad,avgGrad,avgSqGrad,iter+1,lr,beta1,beta2,epsilon);
        iter = iter + 1;

        % éªŒè¯
        Yv_pred = forward(net, dlarray(Xv,"CB"));
        valLoss = mse(Yv_pred, dlarray(Yv,"CB"));

        trainLoss = gather(double(extractdata(loss)));
        valLoss   = gather(double(extractdata(valLoss)));

        yyaxis left;  addpoints(hTrain, iter, trainLoss); addpoints(hVal, iter, valLoss);
        yyaxis right; addpoints(hLR,    iter, lr);
        drawnow limitrate;

        logI(end+1)=iter; logT(end+1)=trainLoss; logV(end+1)=valLoss; logLR(end+1)=lr; %#ok<AGROW>
        if mod(iter,200)==0
            fprintf("  iter=%d  lr=%g  train=%f  val=%f  (gradNorm=%.3f)\n", iter, lr, trainLoss, valLoss, gn);
        end
    end
end
fprintf("Training done.\n\n");

%% ====== è®­ç»ƒåï¼šæ‰“å°è®­ç»ƒæ ·æœ¬ True vs Pred ======
net_cpu = dlupdate(@gather, net);
train_id = choose_random_index(trnIdx, N);
x_tr_dbg  = Xall(:, train_id);
y_tr_true = Yall(:, train_id);
y_tr_pred = predict_raw(net_cpu, single(x_tr_dbg));
diff_abs  = abs(y_tr_pred(:) - y_tr_true(:));
fprintf("[DEBUG] ---- è®­ç»ƒæ ·æœ¬ #%d: true vs pred ----\n", train_id);
fprintf("  max(|diff|) = %g\n", max(diff_abs));
fprintf("  mean(|diff|)= %g\n", mean(diff_abs));
fprintf("  y_tr_true(1:36) = \n"); disp(y_tr_true(:));
fprintf("  y_tr_pred(1:36) = \n"); disp(y_tr_pred(:));
fprintf("[DEBUG] -------------------------------------------\n\n");

%% ====== è‡ªæ£€å¹¶å¯¼å‡º ======
sampleIdx = randi(N);
s0   = sampleStructs{sampleIdx};
x_te = Xall(:,sampleIdx); y_te = Yall(:,sampleIdx);
y_pd = predict_raw(net_cpu, single(x_te));
rmse = sqrt(mean((y_pd(:) - y_te(:)).^2));
fprintf("Final self-check RMSE on 1 sample: %f\n", rmse);

selfInputPath = fullfile(outDir, "selfcheck_input.npz");
save_npz_full(selfInputPath, s0.Xm, s0.Ym, s0.x60, s0.y36, s0.flatOrder, cfg);

selfPredPath  = fullfile(outDir, "selfcheck_pred.npz");
Ym_pred = vec36_to_3mats(y_pd(:), s0.flatOrder);
save_npz_full(selfPredPath, s0.Xm, Ym_pred, s0.x60, y_pd(:), s0.flatOrder, cfg);

fprintf("Self-check npz exported to:\n  %s\n  %s\n\n", selfInputPath, selfPredPath);

%% ====== ä¿å­˜æ¨¡å‹ & ONNX ======
prep.muX  = zeros(60,1); prep.sigX = ones(60,1);
prep.muY  = zeros(36,1); prep.sigY = ones(36,1);
prep.flatOrder = char(flatOrder_all);
fid = fopen(fullfile(outDir,"deformer_mapper_prep.json"),'w');
fwrite(fid, jsonencode(prep)); fclose(fid);

save(fullfile(outDir,"deformer_mapper_dlnet.mat"),"net_cpu","-v7.3");

onnxPath = fullfile(outDir,"deformer_mapper.onnx");
try
    if exist('exportONNXNetwork','file')==2
        lgraph2 = assignLearnables(layerGraph(layers), net_cpu);
        exportONNXNetwork(lgraph2, onnxPath, "OpsetVersion", 17);
        fprintf("ONNX exported: %s\n", onnxPath);
    else
        fprintf("exportONNXNetwork ä¸å­˜åœ¨ï¼Œè·³è¿‡ ONNX å¯¼å‡ºã€‚\n");
    end
catch ME
    warning("å¯¼å‡º ONNX å¤±è´¥ï¼š%s", ME.message);
end

%% ====== æ›²çº¿ & CSV ======
saveas(fig, fullfile(outDir,'combined_loss_curve.png'));
T=table(logI(:),logT(:),logV(:),logLR(:), ...
    'VariableNames',{'iteration','train_loss','val_loss','learning_rate'});
writetable(T, fullfile(outDir,'training_log.csv'));

fprintf("âœ… All done! Output in %s\n", outDir);
fprintf("   Self-check RMSE: %f\n", rmse);
end % ===== ä¸»å‡½æ•° =====

%% ====== å·¥å…·å‡½æ•° ======
function idx = choose_random_index(trnIdx, N)
if isempty(trnIdx), idx = randi(N); else, idx = trnIdx(randi(numel(trnIdx))); end
end

function idx = pick_sample_by_yvec_has_translation(Yall)
idx = 1;
if isempty(Yall), return; end
tran_idx = [4 8 12 16 20 24 28 32 36];
for i = 1:size(Yall,2)
    y = Yall(:,i);
    if any(abs(y(tran_idx)) > 1e-8), idx = i; return; end
end
idx = randi(size(Yall,2));
end

function s = read_single_npz_maya_style(fpath)
np = py.importlib.import_module('numpy');
data = np.load(fpath, pyargs("allow_pickle",true));

if ~isempty(data.get('flatOrder'))
    try, flatOrder = string(char(data.get('flatOrder').tolist()));
    catch, flatOrder = string(char(data.get('flatOrder')));
    end
else
    flatOrder = "row";
end

Xm = []; Ym = [];
if ~isempty(data.get('X_mats')), Xm = double(py.numpy.array(data.get('X_mats'))); end
if ~isempty(data.get('Y_mats')), Ym = double(py.numpy.array(data.get('Y_mats'))); end

hasXvec = ~isempty(data.get('X_vec60'));
hasYvec = ~isempty(data.get('Y_vec36'));

if hasXvec
    xv = data.get('X_vec60');
    x60 = double(py.array.array('d', py.numpy.nditer(xv))).'; x60 = x60(:);
else
    x60 = pack_input_60_from_mats(Xm, flatOrder);
end
if hasYvec
    yv = data.get('Y_vec36');
    y36 = double(py.array.array('d', py.numpy.nditer(yv))).'; y36 = y36(:);
else
    y36 = pack_3mats_to_vec36_from_mats(Ym, flatOrder);
end

if isempty(Xm), Xm = vec60_to_5mats(x60, flatOrder); end
if isempty(Ym), Ym = vec36_to_3mats(y36, flatOrder); end

s = struct('x60',x60,'y36',y36,'Xm',Xm,'Ym',Ym,'flatOrder',flatOrder);
end

function v60 = pack_input_60_from_mats(Xm, flatOrder)
v60 = zeros(60,1);
for i = 1:5
    M34 = Xm(1:3,1:4,i);
    if flatOrder=="row", v = reshape(M34,12,1); else, v = reshape(M34.',12,1); end
    v60((i-1)*12+1:i*12) = v;
end
end

function v36 = pack_3mats_to_vec36_from_mats(Ym, flatOrder)
v36 = zeros(36,1);
for i = 1:3
    M34 = Ym(1:3,1:4,i);
    if flatOrder=="row", v = reshape(M34,12,1); else, v = reshape(M34.',12,1); end
    v36((i-1)*12+1:i*12) = v;
end
end

function Xm = vec60_to_5mats(v60, flatOrder)
v60 = v60(:); Xm = zeros(4,4,5);
for i = 1:5
    seg = v60((i-1)*12+1:i*12);
    if flatOrder=="row", M34 = reshape(seg,[3,4]); else, M34 = reshape(seg,[4,3]).'; end
    M = eye(4); M(1:3,1:4) = M34; Xm(:,:,i) = M;
end
end

function Ym = vec36_to_3mats(v36, flatOrder)
v36 = v36(:); Ym = zeros(4,4,3);
for i = 1:3
    seg = v36((i-1)*12+1:i*12);
    if flatOrder=="row", M34 = reshape(seg,[3,4]); else, M34 = reshape(seg,[4,3]).'; end
    M = eye(4); M(1:3,1:4) = M34; Ym(:,:,i) = M;
end
end

function save_npz_full(fpath, Xm, Ym, x60, y36, flatOrder, cfg)
np = py.importlib.import_module('numpy');
if cfg.roundOutput
    Xm = round(Xm, cfg.roundDigits);
    Ym = round(Ym, cfg.roundDigits);
    x60 = round(x60, cfg.roundDigits);
    y36 = round(y36, cfg.roundDigits);
end
Xm_py = np.array(single(Xm), pyargs('order','F'));   % (4,4,5)
Ym_py = np.array(single(Ym), pyargs('order','F'));   % (4,4,3)
x60_py = np.array(single(reshape(x60,[60 1])));
y36_py = np.array(single(reshape(y36,[36 1])));
fo_py  = py.str(flatOrder);
np.savez(fpath, pyargs( ...
    'X_mats',Xm_py,'Y_mats',Ym_py, ...
    'X_vec60',x60_py,'Y_vec36',y36_py, ...
    'flatOrder',fo_py));
end

function lr = scheduleLR(cfg, iter, totalItersEst)
switch cfg.scheduler
    case "cosine"
        T = max(1,totalItersEst);
        lr = cfg.minLR + 0.5*(cfg.baseLR - cfg.minLR)*(1 + cos(pi*(iter-1)/T));
    case "step"
        steps = floor((iter-1)/max(1,cfg.stepIters));
        lr = cfg.baseLR * (cfg.stepGamma ^ steps);
    otherwise
        lr = cfg.baseLR;
end
end

function [gradients, loss] = modelGradientsL2(net, Xb, Yb, l2Factor)
Yhat = forward(net, Xb);
mseLoss = mse(Yhat, Yb);
reg = 0;
P = net.Learnables;
for i = 1:size(P,1)
    if endsWith(string(P.Parameter(i)),"Weights")
        reg = reg + sum(P.Value{i}.^2,'all');
    end
end
loss = mseLoss + l2Factor*reg;
gradients = dlgradient(loss, net.Learnables);
end

function n = globalGradL2Norm(grad)
% grad æ˜¯ Learnables è¡¨çš„ Value å•å…ƒæ ¼æ„æˆçš„ cell/tableï¼›è®¡ç®—å…¨å±€ L2 èŒƒæ•°
n = 0;
vals = grad.Value;
for i = 1:numel(vals)
    g = vals{i};
    if ~isempty(g)
        n = n + sum(g.^2,'all');
    end
end
n = sqrt(gather(double(n)));
end

function y_pred = predict_raw(net, x)
dlx = dlarray(single(x),"CB");
dly = forward(net, dlx);
y_pred = double(extractdata(dly));
end

function lgraphOut = assignLearnables(lgraphIn, dlnet)
lgraphOut = lgraphIn;
params = dlnet.Learnables;
for i = 1:size(params,1)
    layerName = params.Layer(i);
    paramName = params.Parameter(i);
    val       = gather(extractdata(params.Value{i}));
    hit = strcmp({lgraphOut.Layers.Name}, layerName);
    if any(hit)
        L = lgraphOut.Layers(hit);
        if isprop(L, paramName)
            L.(paramName) = val;
            lgraphOut = replaceLayer(lgraphOut, layerName, L);
        end
    end
end
end
